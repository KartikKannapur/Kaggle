{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MXNet - Gluon Code Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, nd\n",
    "\n",
    "# #Gluon data module to read data\n",
    "from mxnet.gluon import data as gdata\n",
    "\n",
    "# #Neural Network Layers\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "# #Model Parameter Initalizer\n",
    "from mxnet import init\n",
    "\n",
    "# #Gluon module to define loss functions\n",
    "from mxnet.gluon import loss as gloss\n",
    "\n",
    "# #Optimization Algorithm\n",
    "from mxnet.gluon import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X: features\n",
    "y: labels\n",
    "\"\"\"\n",
    "# #Combining the features and labels into a training set\n",
    "dataset = gdata.ArrayDataset(features, labels)\n",
    "\n",
    "# #Randomly reading data in batches - Mini Batch\n",
    "batch_size = 10\n",
    "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# #         INPUT           #\n",
    "# ###########################\n",
    "# #Sequential Container\n",
    "net = nn.Sequential()\n",
    "\n",
    "# ###########################\n",
    "# #      HIDDEN LAYERS      #\n",
    "# ###########################\n",
    "net.add(nn.Dense(256, activation='relu')) # #256 hidden units with a ReLU activation function\n",
    "\n",
    "# #DROPOUT\n",
    "# #We add dropout after each of the fully connected layers\n",
    "# #and specify the dropout probability\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "        # Add a dropout layer after the first fully connected layer\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        # Add a dropout layer after the second fully connected layer\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(10))\n",
    "\n",
    "\n",
    "# ###########################\n",
    "# #         OUTPUT          #\n",
    "# ###########################\n",
    "# #Adding a Dense layer with a scalar output\n",
    "net.add(nn.Dense(1))\n",
    "\n",
    "# #Adding a Dense layer with 10 outputs\n",
    "net.add(nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize Model Parameters\n",
    "\"\"\"\n",
    "Each weight parameter element is randomly sampled at\n",
    "initialization from a normal distribution with zero \n",
    "mean and sigma standard deviation.\n",
    "\n",
    "The bias parameter is initialized to zero by default\n",
    "\"\"\"\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "\"\"\"\n",
    "Each weight parameter element is randomly sampled from\n",
    "a uniform distribution U[-0.07,0.07], with the bias\n",
    "parameter equal to 0\n",
    "\"\"\"\n",
    "net.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Each layer of the network can be selected via indexing - net[i]\n",
    "net[0]\n",
    "\n",
    "# #The weights and biases in each layer of the network\n",
    "w = net[0].weight.data()\n",
    "b = net[0].bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers and Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Block consists of one or more layers.\n",
    "\n",
    "Requirements for a Block are:\n",
    "1. Input data\n",
    "2. `forward` method produces the output\n",
    "3. `backward` method produces the gradient - performed automatically?\n",
    "4. Initialize and store block specific parameters\n",
    "\n",
    "In fact, the Sequential class is derived from the Block class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    # Declare a layer with model parameters. Here, we declare two fully\n",
    "    # connected layers\n",
    "    def __init__(self, **kwargs):\n",
    "        # Call the constructor of the MLP parent class Block to perform the\n",
    "        # necessary initialization. In this way, other function parameters can\n",
    "        # also be specified when constructing an instance, such as the model\n",
    "        # parameter, params, described in the following sections\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(256, activation='relu')  # Hidden layer\n",
    "        self.output = nn.Dense(10)  # Output layer\n",
    "\n",
    "    # Define the forward computation of the model, that is, how to return the\n",
    "    # required model output based on the input x\n",
    "    def forward(self, x):\n",
    "        # #Forward propogation step\n",
    "        return self.output(self.hidden(x))\n",
    "    \n",
    "net = MLP()\n",
    "net.initialize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.L2Loss() # #Squared Loss or L2-norm loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp/partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return -nd.pick(y_hat, y).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Algo: Mini-batch Stochastic Gradient Descent Algorithm\n",
    "\"\"\"\n",
    "The optimization algorithm will iterate over all parameters present\n",
    "in the network\n",
    "\"\"\"\n",
    "trainer = Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* For a pre-defined number of epochs, we make a pass\n",
    "over the dataset that has been sampled via mini-batching\n",
    "the features(X) and the labels(y)\n",
    "* Then, for each mini-batch:\n",
    "- make prediction via `net(X)` and compare it to the label\n",
    "y and compute the loss function in the forward pass\n",
    "- compute gradients via backward pass\n",
    "- update the model parameters via SGD in the `trainer()` method\n",
    "\"\"\"\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(features), labels)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the error in estimating the weights and biases\n",
    "\"\"\"\n",
    "w = net[0].weight.data()\n",
    "print('Error in estimating w', true_w.reshape(w.shape) - w)\n",
    "b = net[0].bias.data()\n",
    "print('Error in estimating b', true_b - b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
